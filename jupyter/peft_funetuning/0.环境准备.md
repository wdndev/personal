# 1.PEFT 概述

PEFT 是 Huggingface 开源的一个参数高效微调库，它提供了最新的参数高效微调技术，并且可以与 Transformers 和 Accelerate 进行无缝集成。

### 支持的高效微调技术

目前支持的一些高效微调方法如下：

1. LoRA: [**LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS**](https://link.zhihu.com/?target=https://arxiv.org/abs/2106.09685)
2. Prefix Tuning: [**Prefix-Tuning: Optimizing Continuous Prompts for Generation**](https://link.zhihu.com/?target=https://aclanthology.org/2021.acl-long.353/) 和 [**P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks**](https://link.zhihu.com/?target=https://arxiv.org/pdf/2110.07602.pdf)
3. P-Tuning: [**GPT Understands, Too**](https://link.zhihu.com/?target=https://arxiv.org/abs/2103.10385)
4. Prompt Tuning: [**The Power of Scale for Parameter-Efficient Prompt Tuning**](https://link.zhihu.com/?target=https://arxiv.org/abs/2104.08691)
5. AdaLoRA: [**Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning**](https://link.zhihu.com/?target=https://arxiv.org/abs/2303.10512)
6. (IA)^3 : [**Infused Adapter by Inhibiting and Amplifying Inner Activations**](https://link.zhihu.com/?target=https://arxiv.org/abs/2205.05638)

具体可通过 PEFT 的源码看到支持的参数高效微调技术类型枚举。

```python
class PeftType(str, enum.Enum):
    PROMPT_TUNING = "PROMPT_TUNING"
    P_TUNING = "P_TUNING"
    PREFIX_TUNING = "PREFIX_TUNING"
    LORA = "LORA"
    ADALORA = "ADALORA"
    ADAPTION_PROMPT = "ADAPTION_PROMPT"
    IA3 = "IA3"
```

# 2.PEFT 运行环境

- NVIDIA驱动、CUDA、Python
- Pytorch、DeepSpeed、PEFT、jupyterlab

# 3.模型准备

本系列文章基础模型均采用bloomz-560m，因此，需预先从 Huggingface 模型仓库下载。

```shell
git clone https://huggingface.co/bigscience/bloomz-560m
```

基础模型结构如下所示：

```shell
BloomForCausalLM(
  (transformer): BloomModel(
    (word_embeddings): Embedding(250880, 1024)
    (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (h): ModuleList(
      (0): BloomBlock(
        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (self_attention): BloomAttention(
          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
          (attention_dropout): Dropout(p=0.0, inplace=False)
        )
        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): BloomMLP(
          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu_impl): BloomGelu()
          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)
        )
      )
      ...
      (23): BloomBlock(
        ...
      )
    )
    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=1024, out_features=250880, bias=False)
)
```

