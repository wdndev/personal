{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ ‡è®°ï¼ˆtokenï¼‰åˆ†ç±» (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets evaluate transformers[sentencepiece]\n",
    "# !pip install accelerate\n",
    "# # To run the training on TPU, you will need to uncomment the following line:\n",
    "# # !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
    "# !apt install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenåˆ†ç±»ã€‚è¿™ä¸ªé€šç”¨ä»»åŠ¡åŒ…æ‹¬ä»»ä½•å¯ä»¥è¡¨è¿°ä¸ºâ€œä¸ºå¥å­ä¸­çš„è¯æˆ–å­—åˆ†é…æ ‡ç­¾â€çš„é—®é¢˜ï¼Œä¾‹å¦‚ï¼š\n",
    "\n",
    "- **å®ä½“å‘½åè¯†åˆ« (NER)**: æ‰¾å‡ºå¥å­ä¸­çš„å®ä½“ï¼ˆå¦‚äººç‰©ã€åœ°ç‚¹æˆ–ç»„ç»‡ï¼‰ã€‚è¿™å¯ä»¥é€šè¿‡ä¸ºæ¯ä¸ªå®ä½“æˆ–â€œæ— å®ä½“â€æŒ‡å®šä¸€ä¸ªç±»åˆ«çš„æ ‡ç­¾ã€‚\n",
    "- **è¯æ€§æ ‡æ³¨ (POS)**: å°†å¥å­ä¸­çš„æ¯ä¸ªå•è¯æ ‡è®°ä¸ºå¯¹åº”äºç‰¹å®šçš„è¯æ€§ï¼ˆå¦‚åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ç­‰ï¼‰ã€‚\n",
    "- **åˆ†å—ï¼ˆchunkingï¼‰**: æ‰¾åˆ°å±äºåŒä¸€å®ä½“çš„Tokenã€‚è¿™ä¸ªä»»åŠ¡(å¯ç»“åˆPOSæˆ–NER)å¯ä»¥ä»»ä½•å°†ä¸€å—Tokenä½œä¸ºåˆ¶å®šä¸€ä¸ªæ ‡ç­¾(é€šå¸¸æ˜¯B -),å¦ä¸€ä¸ªæ ‡ç­¾(é€šå¸¸I -)è¡¨ç¤ºTokenæ˜¯å¦æ˜¯åŒä¸€å—,å’Œç¬¬ä¸‰ä¸ªæ ‡ç­¾(é€šå¸¸æ˜¯O)è¡¨ç¤ºTokenä¸å±äºä»»ä½•å—ã€‚ä¹Ÿå°±æ˜¯æ ‡å‡ºå¥å­ä¸­çš„çŸ­è¯­å—ï¼Œä¾‹å¦‚åè¯çŸ­è¯­ï¼ˆNPï¼‰ï¼ŒåŠ¨è¯çŸ­è¯­ï¼ˆVPï¼‰ç­‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.æ•°æ®å‡†å¤‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å› ä¸ºä¸‹è½½HFç½‘ç«™ä¸Šçš„æ•°æ®é›†ï¼Œè®¾ç½®ä»£ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°†ä½¿ç”¨ CoNLL-2003 æ•°æ®é›†, å…¶ä¸­åŒ…å«æ¥è‡ªè·¯é€ç¤¾çš„æ–°é—»æŠ¥é“ã€‚\n",
    "\n",
    "è¦åŠ è½½ CoNLL-2003 æ•°æ®é›†ï¼Œæˆ‘ä»¬ä½¿ç”¨ æ¥è‡ª ğŸ¤— Datasets åº“çš„ `load_dataset()` æ–¹æ³•ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\dd\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\conll2003\\9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98 (last modified on Wed Feb 28 23:08:48 2024) since it couldn't be found locally at conll2003, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DownloadConfig\n",
    "\n",
    "# config = DownloadConfig(resume_download=True, max_retries=100) install fsspec==2023.9.2\n",
    "\n",
    "raw_datasets = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ£€æŸ¥è¿™ä¸ªå¯¹è±¡å¯ä»¥çœ‹åˆ°å­˜åœ¨å“ªäº›åˆ—ï¼Œä»¥åŠè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ä¹‹é—´æ˜¯å¦‚ä½•åˆ†å‰²çš„:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥çœ‹åˆ°æ•°æ®é›†åŒ…å«ä¹‹å‰æåˆ°çš„ä¸‰ä¸ªä»»åŠ¡çš„æ ‡ç­¾ï¼šNERã€POS å’Œchunkingã€‚ä¸å…¶ä»–æ•°æ®é›†çš„ä¸€ä¸ªå¾ˆå¤§åŒºåˆ«æ˜¯è¾“å…¥æ–‡æœ¬ä¸æ˜¯ä½œä¸ºå¥å­æˆ–æ–‡æ¡£å‘ˆç°çš„ï¼Œè€Œæ˜¯å•è¯åˆ—è¡¨ï¼ˆæœ€åä¸€åˆ—ç§°ä¸º tokens ï¼Œä½†å®ƒåŒ…å«çš„æ˜¯è¿™äº›è¯æ˜¯é¢„å…ˆæ ‡è®°åŒ–çš„è¾“å…¥ï¼Œä»ç„¶éœ€è¦é€šè¿‡æ ‡è®°å™¨è¿›è¡Œå­è¯æ ‡è®°ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”±äºè¦æ‰§è¡Œå‘½åå®ä½“è¯†åˆ«ï¼ŒæŸ¥çœ‹ NER æ ‡ç­¾ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 0, 7, 0, 0, 0, 7, 0, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™ä¸€åˆ—æ˜¯ç±»æ ‡ç­¾çš„åºåˆ—ã€‚å…ƒç´ çš„ç±»å‹åœ¨ `ner_feature` çš„ `feature` å±æ€§ä¸­ï¼Œå¯ä»¥é€šè¿‡æŸ¥çœ‹è¯¥ç‰¹æ€§çš„`names` å±æ€§æ¥è®¿é—®åç§°åˆ—è¡¨:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "ner_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å› æ­¤ï¼Œè¿™ä¸€åˆ—åŒ…å«çš„å…ƒç´ æ˜¯ `ClassLabels` çš„åºåˆ—ã€‚åºåˆ—å…ƒç´ çš„ç±»å‹åœ¨ `ner_feature` çš„ `feature` ä¸­ï¼Œå¯ä»¥é€šè¿‡æŸ¥çœ‹è¯¥`feature` çš„ `names` å±æ€§æ¥è®¿é—®åç§°åˆ—è¡¨:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = ner_feature.feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™äº›æ ‡ç­¾çš„å«ä¹‰ï¼š\n",
    "\n",
    "- `O` è¡¨ç¤ºè¿™ä¸ªè¯ä¸å¯¹åº”ä»»ä½•å®ä½“ã€‚\n",
    "- `B-PER/I-PER` æ„å‘³ç€è¿™ä¸ªè¯å¯¹åº”äºäººåå®ä½“çš„å¼€å¤´/å†…éƒ¨ã€‚\n",
    "- `B-ORG/I-ORG` çš„æ„æ€æ˜¯è¿™ä¸ªè¯å¯¹åº”äºç»„ç»‡åç§°å®ä½“çš„å¼€å¤´/å†…éƒ¨ã€‚\n",
    "- `B-LOC/I-LOC` æŒ‡çš„æ˜¯æ˜¯è¿™ä¸ªè¯å¯¹åº”äºåœ°åå®ä½“çš„å¼€å¤´/å†…éƒ¨ã€‚\n",
    "- `B-MISC/I-MISC` è¡¨ç¤ºè¯¥è¯å¯¹åº”äºä¸€ä¸ªæ‚é¡¹å®ä½“çš„å¼€å¤´/å†…éƒ¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨è§£ç ä¹‹å‰çœ‹åˆ°çš„æ ‡ç­¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU    rejects German call to boycott British lamb . \n",
      "B-ORG O       B-MISC O    O  O       B-MISC  O    O \n"
     ]
    }
   ],
   "source": [
    "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.å¤„ç†æ•°æ®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ–‡æœ¬éœ€è¦è½¬æ¢ä¸ºToken IDï¼Œç„¶åæ¨¡å‹æ‰èƒ½ç†è§£å®ƒä»¬ã€‚ä¸è¿‡åœ¨æ ‡è®°ä»»åŠ¡ä¸­ï¼Œä¸€ä¸ªå¾ˆå¤§çš„åŒºåˆ«æ˜¯æœ‰pre-tokenizedçš„è¾“å…¥ã€‚å¹¸è¿çš„æ˜¯ï¼Œtokenizer APIå¯ä»¥å¾ˆå®¹æ˜“åœ°å¤„ç†è¿™ä¸ªé—®é¢˜;åªéœ€è¦ç”¨ä¸€ä¸ªç‰¹æ®Šçš„tokenizerã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é¦–å…ˆï¼Œåˆ›å»ºtokenizerå¯¹è±¡ã€‚å¦‚å‰æ‰€è¿°ï¼Œå°†ä½¿ç”¨ BERT é¢„è®­ç»ƒæ¨¡å‹ï¼Œå› æ­¤å°†ä»ä¸‹è½½å¹¶ç¼“å­˜å…³è”çš„åˆ†è¯å™¨å¼€å§‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥æ›´æ¢æŠŠ `model_checkpoint` æ›´æ¢ä¸º [Hub](https://huggingface.co/models)ä¸Šçš„ä»»ä½•å…¶ä»–å‹å·ï¼Œæˆ–ä½¿ç”¨æœ¬åœ°ä¿å­˜çš„é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨ã€‚\n",
    "\n",
    "å”¯ä¸€çš„é™åˆ¶æ˜¯åˆ†è¯å™¨éœ€è¦ç”± Tokenizers åº“æ”¯æŒï¼Œæœ‰ä¸€ä¸ª â€œfastâ€ ç‰ˆæœ¬å¯ç”¨ã€‚å¯ä»¥åœ¨è¿™å¼ [è¡¨](https://huggingface.co/docs/transformers/index#supported-frameworks)ä¸Šçœ‹åˆ°æ‰€æœ‰å¸¦æœ‰å¿«é€Ÿç‰ˆæœ¬çš„æ¶æ„ï¼Œæˆ–è€…å¯ä»¥é€šè¿‡æŸ¥çœ‹ `is_fast` å±æ€§æ¥æ£€æµ‹æ­£åœ¨ä½¿ç”¨çš„tokenizerå¯¹è±¡æ˜¯å¦ç”± Tokenizers æ”¯æŒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¦å¯¹é¢„å…ˆæ ‡è®°çš„è¾“å…¥è¿›è¡Œæ ‡è®°ï¼Œåªéœ€æ·»åŠ  `is_split_into_words=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆ†è¯å™¨æ·»åŠ äº†æ¨¡å‹ä½¿ç”¨çš„ç‰¹æ®ŠToken([CLS] åœ¨å¼€å§‹å’Œ[SEP] æœ€å) è€Œå¤§å¤šæ•°å•è¯æœªè¢«ä¿®æ”¹ã€‚ç„¶è€Œï¼Œå•è¯ `lamb`,è¢«åˆ†ä¸ºä¸¤ä¸ªå­å•è¯ `la` å’Œ `##mb`ã€‚è¿™å¯¼è‡´äº†è¾“å…¥å’Œæ ‡ç­¾ä¹‹é—´çš„ä¸åŒ¹é…:æ ‡ç­¾åˆ—è¡¨åªæœ‰9ä¸ªå…ƒç´ ï¼Œè€Œè¾“å…¥ç°åœ¨æœ‰12ä¸ªtoken ã€‚\n",
    "\n",
    "è®¡ç®—ç‰¹æ®ŠTokenå¾ˆå®¹æ˜“(åœ¨å¼€å¤´å’Œç»“å°¾)ï¼Œä½†è¿˜éœ€è¦ç¡®ä¿æ‰€æœ‰æ ‡ç­¾ä¸é€‚å½“çš„å•è¯å¯¹é½ã€‚ å¹¸è¿çš„æ˜¯ï¼Œç”±äºä½¿ç”¨çš„æ˜¯å¿«é€Ÿåˆ†è¯å™¨ï¼Œå› æ­¤å¯ä»¥è®¿é—®Tokenizersè¶…èƒ½åŠ›ï¼Œè¿™æ„å‘³ç€å¯ä»¥è½»æ¾åœ°å°†æ¯ä¸ªtoken æ˜ å°„åˆ°å…¶ç›¸åº”çš„å•è¯:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é€šè¿‡ä¸€ç‚¹ç‚¹å·¥ä½œï¼Œå¯ä»¥æ‰©å±•æ ‡ç­¾åˆ—è¡¨ä»¥åŒ¹é…token ã€‚\n",
    "\n",
    "åº”ç”¨çš„ç¬¬ä¸€æ¡è§„åˆ™æ˜¯ï¼Œç‰¹æ®Štoken çš„æ ‡ç­¾ä¸º `-100` ã€‚è¿™æ˜¯å› ä¸ºé»˜è®¤æƒ…å†µä¸‹ `-100` æ˜¯ä¸€ä¸ªåœ¨æŸå¤±å‡½æ•°ï¼ˆäº¤å‰ç†µï¼‰ä¸­è¢«å¿½ç•¥çš„ç´¢å¼•ã€‚ç„¶åï¼Œæ¯ä¸ª token éƒ½ä¼šè·å¾—ä¸å…¶æ‰€åœ¨å•è¯çš„token ç›¸åŒçš„æ ‡ç­¾ï¼Œå› ä¸ºå®ƒä»¬æ˜¯åŒä¸€å®ä½“çš„ä¸€éƒ¨åˆ†ã€‚å¯¹äºå•è¯å†…éƒ¨ä½†ä¸åœ¨å¼€å¤´çš„Tokenï¼Œå°† `B-` æ›¿æ¢ä¸º `I-` (å› ä¸ºtoken ä¸ä»¥å®ä½“å¼€å¤´):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç¬¬ä¸€å¥è¯æµ‹è¯•ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n",
       "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`align_labels_with_tokens`å‡½æ•°ä¸ºå¼€å¤´å’Œç»“å°¾çš„ä¸¤ä¸ªç‰¹æ®Šæ ‡è®°æ·»åŠ äº† -100 ï¼Œå¹¶ä¸ºåˆ†æˆä¸¤ä¸ªæ ‡è®°çš„å•è¯æ·»åŠ äº†ä¸€ä¸ªæ–°çš„0 ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸ºäº†é¢„å¤„ç†æ•´ä¸ªæ•°æ®é›†ï¼Œéœ€è¦æ ‡è®°æ‰€æœ‰è¾“å…¥å¹¶åœ¨æ‰€æœ‰æ ‡ç­¾ä¸Šåº”ç”¨ `align_labels_with_tokens()` ã€‚ä¸ºäº†åˆ©ç”¨å¿«é€Ÿåˆ†è¯å™¨çš„é€Ÿåº¦ä¼˜åŠ¿ï¼Œæœ€å¥½åŒæ—¶å¯¹å¤§é‡æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œå› æ­¤å°†ç¼–å†™ä¸€ä¸ªå¤„ç†ç¤ºä¾‹åˆ—è¡¨çš„å‡½æ•°å¹¶ä½¿ç”¨å¸¦ `batched=True` æœ‰é€‰é¡¹çš„ `Dataset.map()`æ–¹æ³•. ä¸ä¹‹å‰çš„ç¤ºä¾‹å”¯ä¸€ä¸åŒçš„æ˜¯å½“åˆ†è¯å™¨çš„è¾“å…¥æ˜¯æ–‡æœ¬åˆ—è¡¨ï¼ˆæˆ–è€…åƒä¾‹å­ä¸­çš„å•è¯åˆ—è¡¨ï¼‰æ—¶ `word_ids()` å‡½æ•°éœ€è¦è·å–æƒ³è¦å•è¯çš„ç´¢å¼•çš„IDï¼Œæ‰€ä»¥æ·»åŠ å®ƒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¯·æ³¨æ„ï¼Œè¿˜æ²¡æœ‰å¡«å……è¾“å…¥ï¼›ç¨åä¼šåœ¨ä½¿ç”¨æ•°æ®æ•´ç†å™¨åˆ›å»ºbatchæ—¶è¿™æ ·åšã€‚\n",
    "\n",
    "ç°åœ¨å¯ä»¥ä¸€æ¬¡æ€§å°†æ‰€æœ‰é¢„å¤„ç†åº”ç”¨äºæ•°æ®é›†çš„å…¶ä»–éƒ¨åˆ†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.ä½¿ç”¨ Trainer API å¾®è°ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],\n",
       "        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n",
       "[-100, 1, 2, -100]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MISC': {'precision': 1.0, 'recall': 0.5, 'f1': 0.67, 'number': 2},\n",
       " 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 0.67,\n",
       " 'overall_f1': 0.8,\n",
       " 'overall_accuracy': 0.89}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = labels.copy()\n",
    "predictions[2] = \"O\"\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/sgugger/bert-finetuned-ner/commit/26ab21e5b1568f9afeccdaed2d8715f571d786ed'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sgugger/bert-finetuned-ner-accelerate'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "model_name = \"bert-finetuned-ner-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"bert-finetuned-ner-accelerate\"\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            key: results[f\"overall_{key}\"]\n",
    "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.wait_for_everyone()\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER', 'score': 0.9988506, 'word': 'Sylvain', 'start': 11, 'end': 18},\n",
       " {'entity_group': 'ORG', 'score': 0.9647625, 'word': 'Hugging Face', 'start': 33, 'end': 45},\n",
       " {'entity_group': 'LOC', 'score': 0.9986118, 'word': 'Brooklyn', 'start': 49, 'end': 57}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"huggingface-course/bert-finetuned-ner\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "æ ‡è®°ï¼ˆtokenï¼‰åˆ†ç±» (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
