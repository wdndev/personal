{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"..\")\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.LangChain LLM 接口测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 文心一言测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "\n",
    "# 读取本地/项目的环境变量。\n",
    "\n",
    "# find_dotenv()寻找并定位.env文件的路径\n",
    "# load_dotenv()读取该.env文件，并将其中的环境变量加载到当前的运行环境中\n",
    "# 如果你设置的是全局的环境变量，这行代码则没有任何作用。\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# 获取环境变量 OPENAI_API_KEY\n",
    "wenxin_api_key = os.environ[\"wenxin_api_key\"]\n",
    "wenxin_secret_key = os.environ[\"wenxin_secret_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wenxin_llm import WenxinLLM\n",
    "\n",
    "# print(wenxin_api_key)\n",
    "\n",
    "llm = WenxinLLM(model = \"ERNIE-Bot-turbo\", \n",
    "                       api_key=wenxin_api_key, \n",
    "                       secret_key=wenxin_secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'您好，我是百度研发的知识增强大语言模型，中文名是文心一言，英文名是ERNIE Bot。我能够与人对话互动，回答问题，协助创作，高效便捷地帮助人们获取信息、知识和灵感。\\n\\n如果您有任何问题，请随时告诉我。'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"你是谁\")\n",
    "# wenxin_llm._llm_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 讯飞星火模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "\n",
    "# 读取本地/项目的环境变量。\n",
    "\n",
    "# find_dotenv()寻找并定位.env文件的路径\n",
    "# load_dotenv()读取该.env文件，并将其中的环境变量加载到当前的运行环境中\n",
    "# 如果你设置的是全局的环境变量，这行代码则没有任何作用。\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "appid = os.environ[\"spark_appid\"]     #填写控制台中获取的 APPID 信息\n",
    "api_secret = os.environ[\"spark_api_secret\"]    #填写控制台中获取的 APISecret 信息\n",
    "api_key = os.environ[\"spark_api_key\"]    #填写控制台中获取的 APIKey 信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_llm import SparkLLM\n",
    "\n",
    "# print(api_secret)\n",
    "\n",
    "llm = SparkLLM(model = \"spark\", \n",
    "               appid=appid, \n",
    "               api_secret=api_secret, \n",
    "               api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'您好，我是科大讯飞研发的认知智能大模型，我的名字叫讯飞星火认知大模型。我可以和人类进行自然交流，解答问题，高效完成各领域认知智能需求。'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"你是谁\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 智普AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "\n",
    "# 读取本地/项目的环境变量。\n",
    "\n",
    "# find_dotenv()寻找并定位.env文件的路径\n",
    "# load_dotenv()读取该.env文件，并将其中的环境变量加载到当前的运行环境中\n",
    "# 如果你设置的是全局的环境变量，这行代码则没有任何作用。\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "api_key = os.environ[\"zhipuai_api_key\"]    #填写控制台中获取的 APIKey 信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "llm = ZhipuAILLM(model=\"chatglm_pro\", \n",
    "                 zhipuai_api_key=api_key, \n",
    "                 temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我是一个名为 智谱清言 的人工智能助手，可以叫我小智🤖，是基于清华大学 KEG 实验室和智谱 AI 公司于2023年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"你是谁\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.测试原生接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from call_llm import get_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "\n",
    "# 读取本地/项目的环境变量。\n",
    "\n",
    "# find_dotenv()寻找并定位.env文件的路径\n",
    "# load_dotenv()读取该.env文件，并将其中的环境变量加载到当前的运行环境中\n",
    "# 如果你设置的是全局的环境变量，这行代码则没有任何作用。\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# 获取环境变量 OPENAI_API_KEY\n",
    "openai_api_key = os.environ[\"openai_api_key\"]\n",
    "wenxin_api_key = os.environ[\"wenxin_api_key\"]\n",
    "wenxin_secret_key = os.environ[\"wenxin_secret_key\"]\n",
    "spark_appid = os.environ[\"spark_appid\"]\n",
    "spark_api_secret = os.environ[\"spark_api_secret\"]\n",
    "spark_api_key = os.environ[\"spark_api_key\"]\n",
    "zhipu_api_key = os.environ[\"zhipuai_api_key\"]\n",
    "\n",
    "os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'\n",
    "os.environ[\"HTTP_PROXY\"] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_completion(\"你是谁\",model=\"gpt-3.5-turbo\", api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'您好，我是百度研发的知识增强大语言模型，中文名是文心一言，英文名是ERNIE Bot。我能够与人对话互动，回答问题，协助创作，高效便捷地帮助人们获取信息、知识和灵感。\\n\\n如果您有任何问题，请随时告诉我。'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"你是谁\",model=\"ERNIE-Bot-turbo\", api_key=wenxin_api_key, secret_key=wenxin_secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'您好，我是科大讯飞研发的认知智能大模型，我的名字叫讯飞星火认知大模型。我可以和人类进行自然交流，解答问题，高效完成各领域认知智能需求。'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"你是谁\",model=\"Spark-1.5\", appid=spark_appid, api_key=spark_api_key, api_secret=spark_api_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我是一个名为 智谱清言 的人工智能助手，可以叫我小智🤖，是基于清华大学 KEG 实验室和智谱 AI 公司于2023年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"你是谁\",model=\"chatglm_std\", api_key=zhipu_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.其他测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_to_llm import model_to_llm\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "\n",
    "# 读取本地/项目的环境变量。\n",
    "\n",
    "# find_dotenv()寻找并定位.env文件的路径\n",
    "# load_dotenv()读取该.env文件，并将其中的环境变量加载到当前的运行环境中\n",
    "# 如果你设置的是全局的环境变量，这行代码则没有任何作用。\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# 获取环境变量 OPENAI_API_KEY\n",
    "wenxin_api_key = os.environ[\"wenxin_api_key\"]\n",
    "wenxin_secret_key = os.environ[\"wenxin_secret_key\"]\n",
    "\n",
    "llm = model_to_llm(\"ERNIE-Bot\", temperature=0.2, api_key=wenxin_api_key, Wenxin_secret_key=wenxin_secret_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'您好，我是百度研发的知识增强大语言模型，中文名是文心一言，英文名是ERNIE Bot。我能够与人对话互动，回答问题，协助创作，高效便捷地帮助人们获取信息、知识和灵感。\\n\\n如果您有任何问题，请随时告诉我。'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"你是谁\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.自定义ChatGLM3测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_to_llm import model_to_llm\n",
    "llm = model_to_llm(\"chatglm3\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我是一个名为 ChatGLM3-6B 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"你是谁\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'晚上睡不着，可以尝试以下方法来帮助入睡：\\n\\n1. 保持放松：尝试通过深呼吸、渐进性肌肉松弛或冥想等方法来放松身心。\\n\\n2. 改善睡眠环境：确保卧室安静、舒适且黑暗。避免在睡前使用电子设备，因为它们发出的蓝光可能会抑制褪黑激素的分泌，影响睡眠质量。\\n\\n3. 规律作息：尽量保持作息规律，每天按时上床睡觉和起床。这有助于调整生物钟，提高睡眠质量。\\n\\n4. 避免刺激性活动：睡前一小时内避免进行剧烈运动、观看紧张刺激的电影或进行其他可能让身体兴奋的活动。\\n\\n5. 喝点助眠茶：有些茶叶具有助眠作用，如红茶、绿茶、薰衣草茶等。然而，咖啡因和酒精会影响睡眠质量，因此要避免在睡前喝含有咖啡因和酒精的饮料。\\n\\n6. 规律作息：尽量保持作息规律，每天按时上床睡觉和起床。这有助于调整生物钟，提高睡眠质量。\\n\\n7. 睡前放松：尝试在睡前进行放松活动，如阅读、听轻音乐等，有助于缓解紧张情绪，进入放松状态。\\n\\n8. 避免过度疲劳：确保白天充分休息，避免在疲劳状态下进行剧烈运动或进行其他可能让身体感到疲惫的活动。\\n\\n9. 睡前限制饮水：避免在睡前喝太多水，以免夜间因排尿次数增加而影响睡眠。\\n\\n10. 咨询专业人士：如果长时间存在睡眠问题，可以考虑咨询专业医生，了解可能的原因并寻求相应的治疗建议。\\n\\n以上方法可能对您有所帮助，但请因人而异，找到最适合自己的入睡方式。'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"晚上睡不着应该怎么办\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'`new` 和 `malloc` 都是用于动态分配内存的函数，但它们在实现和用法上有一些区别。\\n\\n1. 实现方式：\\n   - `new` 是 C 语言中的原版，它在 C 语言中是作为语言规范的一部分提供的。`new` 会在运行时检查内存分配是否成功，如果分配失败，会抛出异常。\\n   - `malloc` 是 C++ 中的库函数，用于动态分配内存。`malloc` 会在运行时检查内存分配是否成功，如果分配失败，会抛出异常。与 `new` 不同的是，`malloc` 会在调用 `new` 之前释放已分配的内存。\\n\\n2. 参数：\\n   - `new` 没有参数，用于分配内存的函数会自动传递所需的参数。\\n   - `malloc` 有一个参数，用于指定要分配的内存大小。如果分配的内存大于或等于所需大小，`malloc` 会返回一个指向分配内存的指针；否则，会抛出异常。\\n\\n3. 返回值：\\n   - `new` 返回一个指向分配内存的指针。\\n   - `malloc` 返回一个指向分配内存的指针，如果分配失败，会抛出异常。\\n\\n4. 异常处理：\\n   - `new` 不会抛出异常，如果分配内存失败，可以调用 `delete` 或其他相关函数来释放内存。\\n   - `malloc` 在分配内存失败时抛出异常，需要使用 `std::runtime_error` 或 `std::bad_alloc` 等异常处理函数进行处理。\\n\\n5. 指针用法：\\n   - 在 `new` 中，需要手动调用 `delete` 或其他相关函数来释放分配的内存。\\n   - 在 `malloc` 中，需要手动调用 `free` 函数来释放分配的内存。\\n\\n总结起来，`new` 和 `malloc` 都是用于动态分配内存的函数，但 `new` 是 C 语言中的原版，而 `malloc` 是 C++ 中的库函数。在 `new` 中，分配的内存需要手动释放，而在 `malloc` 中，分配的内存会在调用 `free` 函数后自动释放。'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"new 和 malloc 的区别有哪些\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我是一个名为 ChatGLM3-6B 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from call_llm import get_completion\n",
    "get_completion(\"你是谁\", model=\"chatglm3\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
